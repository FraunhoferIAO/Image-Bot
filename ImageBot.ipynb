{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Setup the relevant variables for your training data generation pipeline. The following variables can be set:\n",
    "\n",
    "- project_name: The name of the project for which you will generate the training data set. One project contains multiple objects (see object_name), for which training data is generated. For each project a project folder is genearted under \"output\" to store the object's data.\n",
    "- object_name: The name of the object you are currently capturing with this jupyter notebook. Each object's data will be placed in a separate folder within your project's folder. If you set the project name and the object name to names which you used previously, you can extend an already existing dataset or process it further.\n",
    "- object_position: The center of the object, relative to the robot arm in mm.\n",
    "- arduino_serial_port: The name of the Serial Port to which the Braccio Robot Arm's Arduino is connected. Normally, this is something like 'Com1' (Windows) or '/dev/ttyUSB0' (Ubuntu). If you don't know how to find the correct Serial Port, have a look in the [User Manual](doc/UserManual.md).\n",
    "- camera: The index of the camera to use. If your computer doesn't have a built-in camera, you might need to put 0 here. If your computer has a built-in camera, the index to use might be most probably 1. Otherwise, just check the values in between 0 and 5 to figure out, which camera is the right one to choose.\n",
    "- different_backgrounds (optional): Parameter to set with how many different backgrounds each foreground image should be blended. Increase this value if you need more training data and decrease it to have less training data. Must be greater or equal to 1.\n",
    "- prevent_blurred_objects (optional): Some objects with a very uniform texture might be blurred when blending with the background image. This variable can help to prevent it.\n",
    "\n",
    "Always run this part before running any other section of this notebook!\n",
    "\n",
    "To run each part in the following, please click in the code block to run and then on the play icon in the main menu bar on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please insert your project and object name here\n",
    "project_name = \"TestPaper\"\n",
    "object_name = \"Tissue\"\n",
    "object_position = [600, 0, 50]\n",
    "arduino_serial_port = \"Com1\"\n",
    "camera = 1\n",
    "\n",
    "# Optional parameters\n",
    "different_backgrounds = 2\n",
    "prevent_blurred_objects = True\n",
    "\n",
    "\n",
    "# ======================================= # Do not alter anything below this line! =======================================\n",
    "import os\n",
    "project_folder = os.path.join(\"./output\", project_name)\n",
    "object_folder = os.path.join(project_folder, object_name)\n",
    "raw_folder = os.path.join(object_folder, \"0_raw\")\n",
    "masked_folder = os.path.join(object_folder, \"1_masked\")\n",
    "blended_folder = os.path.join(object_folder, \"2_blended\")\n",
    "yolo_folder = os.path.join(object_folder, \"3_yolo\")\n",
    "\n",
    "from ImageBot.Config import *\n",
    "SERIAL_PORT = arduino_serial_port\n",
    "CAMERA_ID = camera\n",
    "MODEL_MULTIPLY_MESSAGE_BACKGROUND_ASSIGNMENT = different_backgrounds\n",
    "PREVENT_BLURRED_OBJECTS = prevent_blurred_objects\n",
    "\n",
    "print(\"Finished successfull! Please proceed with the following steps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Adjust Camera Settings\n",
    "\n",
    "Only call this part if you want to adjust your camera settings. It only works for **Microsoft Windows**. Otherwise, you can skip this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "if os.name == 'nt':\n",
    "    vid = cv2.VideoCapture(1, cv2.CAP_DSHOW)\n",
    "    vid.set(cv2.CAP_PROP_SETTINGS, 3)\n",
    "    input(\"Adjust your camera settings and hit enter to continue\")\n",
    "\n",
    "    print(\"Camera resolution is:\", vid.get(cv2.CAP_PROP_FRAME_WIDTH), \"x\", vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    while vid.isOpened():\n",
    "        ret, img = vid.read()\n",
    "        cv2.imshow(\"Image\", img)\n",
    "        if cv2.waitKey(33) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Aquisition\n",
    "\n",
    "This section aquires the raw images of the objects in front of the greenscreen. Please place your object properly and make sure the Arduino for the robot is connected, before starting this section.\n",
    "\n",
    "The script will control the robot to take pictures of your object from different vertical perspectives. After each run a prompt is shown, which asks, whether you want to rotate the object by hand and do another data aquisition sequence from a different horizontal perspective.\n",
    "**Make sure to capture enough different perspectives of the object to be recognized!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ImageBot.data_aquisition.DataAquisitionSequence import DataAquistionSequence\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def image_taken_clbck(image):\n",
    "    print(\"Image %s has been taken!\")\n",
    "    cv2.imshow('bounding box', image)\n",
    "    # Wait until destruction\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "seq = DataAquistionSequence(raw_folder)\n",
    "seq.init(SERIAL_PORT, CAMERA_ID)\n",
    "\n",
    "run = True\n",
    "while run:\n",
    "    run_next = input(\"Do you want to run the data aquisition (press enter for yes, any letter for quit):\")\n",
    "    if not run_next:\n",
    "        seq.aquire(object_position, 50, 300, 5, image_taken_clbck)\n",
    "    else:\n",
    "        run = False\n",
    "        \n",
    "seq.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Greenscreen Removal\n",
    "\n",
    "All captured images are shown in sequential order. For each image select a green value via left-click with the mouse in the image. After green value selection, the associated mask is shown. It will disappear after 1 sec or whenever you press any key. After that you can:\n",
    "\n",
    "- Select a better green value via left mouse click again\n",
    "- Confirm your green value selection by pressing any key (except 'x')\n",
    "- Skip this image by pressing the 'x' key (Notice: This image will then not be processed any further)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ImageBot.image_processing import PostProcessor as post\n",
    "from pathlib import Path\n",
    "from ImageBot.infrastructure.ImageMessage import ImageMessage\n",
    "from ImageBot.infrastructure.filter import show, load_image\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "\n",
    "post.init(dest_folder=Path(masked_folder))\n",
    "post.load_images(Path(raw_folder))\n",
    "\n",
    "progress = tqdm(total=post.Loader.qsize())\n",
    "def clbck(message):\n",
    "    progress.update()\n",
    "\n",
    "while not post.Loader.empty():\n",
    "    result : ImageMessage = ImageMessage(uuid.uuid4())\n",
    "    load_image(result, post.Loader)\n",
    "    post.PostProcessor.execute(result, clbck=clbck)\n",
    "\n",
    "post.PostProcessor.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Blending\n",
    "\n",
    "This part will blend the foreground image with different background images. Just start this process - no interaction is necessary.\n",
    "\n",
    "**!Warning!** This section uses highly parallelized processing and thus will use 100% of your CPU. Do not start, if you want to do other things with your computer in the meantime. ;-)\n",
    "\n",
    "*If you don't have a physical setup but want to test the Image-Bot with our datasets, copy the objects in the \"output\" folder, insert the correct names in Step \"0. Setup\" and then directly proceed with this step.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ImageBot.data_augmentation import AugmentationPipeline as aug\n",
    "\n",
    "from pathlib import Path\n",
    "from ImageBot.infrastructure.ImageMessage import ImageMessage\n",
    "from ImageBot.infrastructure.filter import show, load_image\n",
    "import uuid\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "aug.load_images(source_folder=Path(masked_folder), bgs_folder=Path('./bgs/'))\n",
    "aug.init(dest_folder=Path(blended_folder))\n",
    "\n",
    "progress = tqdm(total=aug.Loader.qsize())\n",
    "\n",
    "def clbck(message):\n",
    "    progress.update()\n",
    "\n",
    "while not aug.Loader.empty():\n",
    "    result : ImageMessage = ImageMessage(uuid.uuid4())\n",
    "    load_image(result, aug.Loader, load_mask=True)\n",
    "    aug.AugmentationPipeline.execute(result, clbck=clbck)\n",
    "\n",
    "aug.AugmentationPipeline.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert to YOLO dataset\n",
    "\n",
    "This part converts your generated and labled images to a format which can directly be used to train YOLO image detection algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ImageBot.to_yolo.YoloConverter import to_yolo_dataset\n",
    "\n",
    "to_yolo_dataset(blended_folder, yolo_folder, test_training_split=0.3)\n",
    "print(\"Finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train you model\n",
    "\n",
    "Take the yolo folder for each of your objects and use it in your machine training environment to train your algorithm!\n",
    "Have fun! :-)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62863482d9ce21b90c82c03b25704679b16cec1090372b15443a20e9e6f51727"
  },
  "kernelspec": {
   "display_name": "image-bot-kernel",
   "language": "python",
   "name": "image-bot-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
